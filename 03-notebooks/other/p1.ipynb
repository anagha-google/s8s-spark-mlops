{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c776c6-9f5d-4378-b8af-9f29b8b9f246",
   "metadata": {},
   "source": [
    "# MLOps for Spark MLLib with Vertex AI Pipelines - Part 1\n",
    "In this notebook, we create a custom docker image for Serverless Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5ee649-155a-43e8-a82a-c271b14efa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path as path\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Condition, Input,\n",
    "                        Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac79150-124b-41cc-841f-bd149a867478",
   "metadata": {},
   "source": [
    "## 1. Install packages for Vertex AI capabilties of interest\n",
    "This is a one-time activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25365d8-8427-464b-b84a-a31856f215c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif not os.getenv(\"IS_TESTING\"):\\n    # Automatically restart kernel after installs\\n    import IPython\\n\\n    app = IPython.Application.instance()\\n    app.kernel.do_shutdown(True)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip3 install --user --upgrade google-cloud-aiplatform==1.11.0 kfp==1.8.11 google-cloud-pipeline-components==1.0.1 --quiet --no-warn-conflicts\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "\"\"\"\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397385b-77cf-47d4-8757-011976e14413",
   "metadata": {},
   "source": [
    "## 2. Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffa4d18-78e7-4959-b41a-283f9a34de06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  s8s-spark-ml-mlops\n",
      "Project Number:  974925525028\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "PROJECT_NBR = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = project_id_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
    "    PROJECT_NBR = project_nbr_output[0]\n",
    "    print(\"Project Number: \", PROJECT_NBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d955f1-0b27-4ff5-99b5-1b1c273d4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09092aa4-6319-4e3c-b7ff-9415c1a82ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_BUCKET_URI = \"gs://s8s_code_bucket-\" + PROJECT_NBR + \"/\"  \n",
    "REGION = \"us-central1\"  \n",
    "GCR_REPO_NM = \"s8s-spark-\" + PROJECT_NBR\n",
    "UMSA = !gcloud config list account --format \"value(core.account)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d289eb33-6148-4e4b-89d3-30f31850b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_SCRATCH_DIR = path(\"docker-build-scratch\")\n",
    "DOCKER_IMAGE_TAG = \"1.0.3\"\n",
    "DOCKER_IMAGE_NM = \"s8s-sparkml-serve\"\n",
    "DOCKER_IMAGE_FQN = f\"gcr.io/{PROJECT_ID}/{DOCKER_IMAGE_NM}:{DOCKER_IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad08be-9957-4795-bf10-4cd491669399",
   "metadata": {},
   "source": [
    "## 3. Create customer Container Image for Serverless Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430dddb-2c05-4683-a289-3a5f59fed7f3",
   "metadata": {},
   "source": [
    "#### 3.1. Create a local scratch directory (if not exists) and clean up any previously created artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d622e5-72f1-425e-a1cf-2073f00ed105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c351fb17-90a7-4b12-be8f-dd3a192e0b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 99960\n",
      "drwxrwxrwx  3 jupyter jupyter     4096 Jul 27 03:54 .\n",
      "drwxr-xr-x 12 jupyter jupyter     4096 Jul 27 04:12 ..\n",
      "drwxr-xr-x  2 jupyter jupyter     4096 Jul 25 18:47 .ipynb_checkpoints\n",
      "-rw-r--r--  1 jupyter jupyter     3075 Jul 27 03:54 Dockerfile\n",
      "-rw-r--r--  1 jupyter jupyter 66709754 Jul 21  2021 Miniconda3-py39_4.10.3-Linux-x86_64.sh\n",
      "-rw-r--r--  1 jupyter jupyter    10332 Jul 26 17:04 pipeline_1032.json\n",
      "-rw-r--r--  1 jupyter jupyter    22920 Jul 26 22:51 pipeline_1544.json\n",
      "-rw-r--r--  1 jupyter jupyter    13731 Jul 26 21:28 pipeline_1891.json\n",
      "-rw-r--r--  1 jupyter jupyter    10373 Jul 25 20:41 pipeline_2265.json\n",
      "-rw-r--r--  1 jupyter jupyter    10352 Jul 26 17:13 pipeline_2426.json\n",
      "-rw-r--r--  1 jupyter jupyter    10332 Jul 26 16:49 pipeline_2458.json\n",
      "-rw-r--r--  1 jupyter jupyter    13748 Jul 26 20:11 pipeline_3517.json\n",
      "-rw-r--r--  1 jupyter jupyter    13731 Jul 26 21:32 pipeline_4152.json\n",
      "-rw-r--r--  1 jupyter jupyter    10332 Jul 26 16:46 pipeline_6117.json\n",
      "-rw-r--r--  1 jupyter jupyter    10373 Jul 25 23:03 pipeline_6710.json\n",
      "-rw-r--r--  1 jupyter jupyter    10332 Jul 26 04:29 pipeline_7209.json\n",
      "-rw-r--r--  1 jupyter jupyter    22860 Jul 26 22:14 pipeline_7387.json\n",
      "-rw-r--r--  1 jupyter jupyter    22915 Jul 26 22:41 pipeline_8665.json\n",
      "-rw-r--r--  1 jupyter jupyter    10332 Jul 26 15:52 pipeline_8866.json\n",
      "-rw-r--r--  1 jupyter jupyter    22915 Jul 26 22:27 pipeline_9693.json\n",
      "-rw-r--r--  1 jupyter jupyter 35385228 Jul 22 19:26 spark-bigquery-with-dependencies_2.12-0.22.2.jar\n"
     ]
    }
   ],
   "source": [
    "!ls -al $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e4fe5e-e807-47e6-b48f-f7c71ab64a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_SCRATCH_DIR/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c328332-7576-483b-bd4b-ede36ffc12be",
   "metadata": {},
   "source": [
    "#### 3.2. Download the jars/files to be baked into the container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ab87ee-8681-4cbc-98a8-4a3073d93390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar...\n",
      "- [1 files][ 33.8 MiB/ 33.8 MiB]                                                \n",
      "Operation completed over 1 objects/33.8 MiB.                                     \n",
      "--2022-07-27 13:56:01--  https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh\n",
      "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
      "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66709754 (64M) [application/x-sh]\n",
      "Saving to: ‘docker-build-scratch/Miniconda3-py39_4.10.3-Linux-x86_64.sh’\n",
      "\n",
      "Miniconda3-py39_4.1 100%[===================>]  63.62M  70.9MB/s    in 0.9s    \n",
      "\n",
      "2022-07-27 13:56:02 (70.9 MB/s) - ‘docker-build-scratch/Miniconda3-py39_4.10.3-Linux-x86_64.sh’ saved [66709754/66709754]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar $LOCAL_SCRATCH_DIR/\n",
    "!wget -P $LOCAL_SCRATCH_DIR https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c5931-3eca-40a3-ab16-3e288d1354fd",
   "metadata": {},
   "source": [
    "#### 3.3. Create Dockerfile and persist to local scratch directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6cc787-2506-4157-9ba8-a2fbb3ddac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_SCRATCH_DIR/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "452362a0-a4d7-4c95-a776-ea6a9246bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerFileContent = \"\"\"\n",
    "# Debian 11 is recommended.\n",
    "FROM debian:11-slim\n",
    "\n",
    "# Suppress interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# (Required) Install utilities required by Spark scripts.\n",
    "RUN apt update && apt install -y procps tini\n",
    "\n",
    "# (Optional) Add extra jars.\n",
    "ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
    "ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
    "RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "\n",
    "# (Optional) Install and configure Miniconda3.\n",
    "ENV CONDA_HOME=/opt/miniconda3\n",
    "ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
    "ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
    "COPY Miniconda3-py39_4.10.3-Linux-x86_64.sh .\n",
    "RUN bash Miniconda3-py39_4.10.3-Linux-x86_64.sh -b -p /opt/miniconda3 \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
    "\n",
    "# (Optional) Install Conda packages.\n",
    "#\n",
    "# The following packages are installed in the default image, it is strongly\n",
    "# recommended to include all of them.\n",
    "#\n",
    "# Use mamba to install packages quickly.\n",
    "RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\n",
    "    && ${CONDA_HOME}/bin/mamba install \\\n",
    "      conda \\\n",
    "      cython \\\n",
    "      fastavro \\\n",
    "      fastparquet \\\n",
    "      gcsfs \\\n",
    "      google-cloud-bigquery-storage \\\n",
    "      google-cloud-bigquery[pandas] \\\n",
    "      google-cloud-bigtable \\\n",
    "      google-cloud-container \\\n",
    "      google-cloud-datacatalog \\\n",
    "      google-cloud-dataproc \\\n",
    "      google-cloud-datastore \\\n",
    "      google-cloud-language \\\n",
    "      google-cloud-logging \\\n",
    "      google-cloud-monitoring \\\n",
    "      google-cloud-pubsub \\\n",
    "      google-cloud-redis \\\n",
    "      google-cloud-spanner \\\n",
    "      google-cloud-speech \\\n",
    "      google-cloud-storage \\\n",
    "      google-cloud-texttospeech \\\n",
    "      google-cloud-translate \\\n",
    "      google-cloud-vision \\\n",
    "      koalas \\\n",
    "      matplotlib \\\n",
    "      mleap \\\n",
    "      nltk \\\n",
    "      numba \\\n",
    "      numpy \\\n",
    "      openblas \\\n",
    "      orc \\\n",
    "      pandas \\\n",
    "      pyarrow \\\n",
    "      pysal \\\n",
    "      pytables \\\n",
    "      python \\\n",
    "      regex \\\n",
    "      requests \\\n",
    "      rtree \\\n",
    "      scikit-image \\\n",
    "      scikit-learn \\\n",
    "      scipy \\\n",
    "      seaborn \\\n",
    "      sqlalchemy \\\n",
    "      sympy \\\n",
    "      virtualenv\n",
    "\n",
    "# (Optional) Add extra Python modules.\n",
    "#ENV PYTHONPATH=/opt/python/packages\n",
    "#RUN mkdir -p \"${PYTHONPATH}\"\n",
    "#COPY test_util.py \"${PYTHONPATH}\"\n",
    "\n",
    "# (Optional) Install R and R libraries.\n",
    "RUN apt update \\\n",
    "  && apt install -y gnupg \\\n",
    "  && apt-key adv --no-tty \\\n",
    "      --keyserver \"hkp://keyserver.ubuntu.com:80\" \\\n",
    "      --recv-keys 95C0FAF38DB3CCAD0C080A7BDC78B2DDEABC47B7 \\\n",
    "  && echo \"deb http://cloud.r-project.org/bin/linux/debian bullseye-cran40/\" \\\n",
    "      >/etc/apt/sources.list.d/cran-r.list \\\n",
    "  && apt update \\\n",
    "  && apt install -y \\\n",
    "      libopenblas-base \\\n",
    "      libssl-dev \\\n",
    "      r-base \\\n",
    "      r-base-dev \\\n",
    "      r-recommended \\\n",
    "      r-cran-blob\n",
    "\n",
    "ENV R_HOME=/usr/lib/R\n",
    "\n",
    "# (Required) Create the 'spark' group/user.\n",
    "# The GID and UID must be 1099. Home directory is required.\n",
    "RUN groupadd -g 1099 spark\n",
    "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
    "USER spark\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{LOCAL_SCRATCH_DIR}/Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerFileContent)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "494b3ab3-1681-4936-ab9d-6796bbee150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 99720\n",
      "drwxrwxrwx  3 jupyter jupyter     4096 Jul 27 13:56 .\n",
      "drwxr-xr-x 12 jupyter jupyter     4096 Jul 27 04:12 ..\n",
      "drwxr-xr-x  2 jupyter jupyter     4096 Jul 25 18:47 .ipynb_checkpoints\n",
      "-rw-r--r--  1 jupyter jupyter     3075 Jul 27 13:56 Dockerfile\n",
      "-rw-r--r--  1 jupyter jupyter 66709754 Jul 21  2021 Miniconda3-py39_4.10.3-Linux-x86_64.sh\n",
      "-rw-r--r--  1 jupyter jupyter 35385228 Jul 27 13:56 spark-bigquery-with-dependencies_2.12-0.22.2.jar\n"
     ]
    }
   ],
   "source": [
    "!ls -al $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa6314-acb6-4b84-9724-8feca0a0fec1",
   "metadata": {},
   "source": [
    "#### 3.4. Build an image and register into the Artifact registry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b590f7-8efe-492a-8f72-50fd4a0de97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud builds submit --tag $DOCKER_IMAGE_FQN $LOCAL_SCRATCH_DIR --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97dc2758-1edc-49d5-99ef-3bb2d7d7e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e4eaa47-4610-49df-b2e1-655f3a21f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/s8s-spark-ml-mlops/s8s-sparkml-serve:1.0.3\n"
     ]
    }
   ],
   "source": [
    "print(DOCKER_IMAGE_FQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ff8763-48d9-4d3e-ab58-101f663d084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  102.1MB\n",
      "Step 1/18 : FROM debian:11-slim\n",
      " ---> f8f4b4b67518\n",
      "Step 2/18 : ENV DEBIAN_FRONTEND=noninteractive\n",
      " ---> Using cache\n",
      " ---> 72e2721130e4\n",
      "Step 3/18 : RUN apt update && apt install -y procps tini\n",
      " ---> Using cache\n",
      " ---> 5c583e67cb2b\n",
      "Step 4/18 : ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
      " ---> Using cache\n",
      " ---> 2b07abc94b1a\n",
      "Step 5/18 : ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
      " ---> Using cache\n",
      " ---> a6241159484d\n",
      "Step 6/18 : RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
      " ---> Using cache\n",
      " ---> 26fbf7739ffa\n",
      "Step 7/18 : COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
      " ---> Using cache\n",
      " ---> 38b8b6757804\n",
      "Step 8/18 : ENV CONDA_HOME=/opt/miniconda3\n",
      " ---> Using cache\n",
      " ---> e2c5e418255f\n",
      "Step 9/18 : ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
      " ---> Using cache\n",
      " ---> 69df30d56c7f\n",
      "Step 10/18 : ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> a7d5a470069b\n",
      "Step 11/18 : COPY Miniconda3-py39_4.10.3-Linux-x86_64.sh .\n",
      " ---> Using cache\n",
      " ---> 93db01d80472\n",
      "Step 12/18 : RUN bash Miniconda3-py39_4.10.3-Linux-x86_64.sh -b -p /opt/miniconda3   && ${CONDA_HOME}/bin/conda config --system --set always_yes True   && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False   && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge   && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
      " ---> Using cache\n",
      " ---> e9d2cca60962\n",
      "Step 13/18 : RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge     && ${CONDA_HOME}/bin/mamba install       conda       cython       fastavro       fastparquet       gcsfs       google-cloud-bigquery-storage       google-cloud-bigquery[pandas]       google-cloud-bigtable       google-cloud-container       google-cloud-datacatalog       google-cloud-dataproc       google-cloud-datastore       google-cloud-language       google-cloud-logging       google-cloud-monitoring       google-cloud-pubsub       google-cloud-redis       google-cloud-spanner       google-cloud-speech       google-cloud-storage       google-cloud-texttospeech       google-cloud-translate       google-cloud-vision       koalas       matplotlib       mleap       nltk       numba       numpy       openblas       orc       pandas       pyarrow       pysal       pytables       python       regex       requests       rtree       scikit-image       scikit-learn       scipy       seaborn       sqlalchemy       sympy       virtualenv\n",
      " ---> Running in 545190ef07b0\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0mCollecting package metadata (repodata.json): ...working... \u001b[91mKilled\n",
      "\u001b[0mThe command '/bin/sh -c ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge     && ${CONDA_HOME}/bin/mamba install       conda       cython       fastavro       fastparquet       gcsfs       google-cloud-bigquery-storage       google-cloud-bigquery[pandas]       google-cloud-bigtable       google-cloud-container       google-cloud-datacatalog       google-cloud-dataproc       google-cloud-datastore       google-cloud-language       google-cloud-logging       google-cloud-monitoring       google-cloud-pubsub       google-cloud-redis       google-cloud-spanner       google-cloud-speech       google-cloud-storage       google-cloud-texttospeech       google-cloud-translate       google-cloud-vision       koalas       matplotlib       mleap       nltk       numba       numpy       openblas       orc       pandas       pyarrow       pysal       pytables       python       regex       requests       rtree       scikit-image       scikit-learn       scipy       seaborn       sqlalchemy       sympy       virtualenv' returned a non-zero code: 137\n"
     ]
    }
   ],
   "source": [
    "# Build the image - started at 1:50 PM\n",
    "!cd $LOCAL_SCRATCH_DIR && docker build . --progress=tty -f Dockerfile -t $DOCKER_IMAGE_FQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f8fdf5-a6b5-434e-91a7-b348aac51407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/s8s-spark-ml-mlops/s8s-sparkml-serve]\n",
      "An image does not exist locally with the tag: gcr.io/s8s-spark-ml-mlops/s8s-sparkml-serve\n"
     ]
    }
   ],
   "source": [
    "!docker push $DOCKER_IMAGE_FQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00aeca83-a90c-4426-a042-96c8a733d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                     TAG       IMAGE ID       CREATED         SIZE\n",
      "gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime   1.0.2     36bbe54c9077   4 days ago      6.71GB\n",
      "gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime   1.0.0     275372c9c459   4 days ago      6.71GB\n",
      "<none>                                                         <none>    89dfecaa665e   4 days ago      5.34GB\n",
      "debian                                                         11-slim   f8f4b4b67518   2 weeks ago     80.4MB\n",
      "gcr.io/inverting-proxy/agent                                   <none>    fe507176d0e6   17 months ago   1.73GB\n"
     ]
    }
   ],
   "source": [
    "!docker image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f43cb7-5823-40d0-8372-a45e9e9445c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
