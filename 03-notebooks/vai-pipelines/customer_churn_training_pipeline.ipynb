{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a212fa6e-2198-41c6-9e69-8d7abb407684",
   "metadata": {},
   "source": [
    "# MLOps with Spark MLLib & Vertex AI Pipelines\n",
    "In this notebook, we create a Vertex AI pipeline for MLOps with Spark MLLib powered by Dataproc Serverless Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5a414-f457-4e23-8c8e-62200dc6cfe7",
   "metadata": {},
   "source": [
    "### One time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "548c98b0-89c9-4eca-bd1d-a39ac780c747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip3 install --user --upgrade google-cloud-aiplatform==1.11.0 kfp==1.8.11 google-cloud-pipeline-components==1.0.1 --quiet --no-warn-conflicts\\n\\n# Automatically restart kernel after installs\\n\\nif not os.getenv(\"IS_TESTING\"):\\n    # Automatically restart kernel after installs\\n    import IPython\\n\\n    app = IPython.Application.instance()\\n    app.kernel.do_shutdown(True)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip3 install --user --upgrade google-cloud-aiplatform==1.11.0 kfp==1.8.11 google-cloud-pipeline-components==1.0.1 --quiet --no-warn-conflicts\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46e6a9-0c11-42fe-9595-3007cfcfb0ef",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d34ff675-62e7-410f-b619-f1f8d8ab0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path as path\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Condition, Input,\n",
    "                        Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a129398-d241-47b8-8063-73b605a936d8",
   "metadata": {},
   "source": [
    "#### a. Project specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ed589bb-8ed3-4cb0-96c4-462a310d1741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  spark-s8s-mlops\n",
      "Project Number:  505815944775\n",
      "UMSA FQN:  s8s-lab-sa@spark-s8s-mlops.iam.gserviceaccount.com\n",
      "UNIQUE ID:  9922\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "PROJECT_NBR = \"\"\n",
    "UNIQUE_ID = random.randint(1, 10000)\n",
    "WITHOUT_TASK_CACHING = True\n",
    "BYO_NETWORK = True\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = project_id_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n",
    "    project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
    "    PROJECT_NBR = project_nbr_output[0]\n",
    "    print(\"Project Number: \", PROJECT_NBR)\n",
    "    \n",
    "umsa_output = !gcloud config list account --format \"value(core.account)\"\n",
    "UMSA_FQN = umsa_output[0]\n",
    "print(\"UMSA FQN: \", UMSA_FQN)\n",
    "print(\"UNIQUE ID: \", UNIQUE_ID)\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040cba-24ce-4550-93a7-b851d732f301",
   "metadata": {},
   "source": [
    "#### b. Local resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a231439c-6ae0-4847-8c3f-8ee23c4c5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_BASE_NM = \"customer-churn-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96e1340b-3af1-4f95-a763-b8a1e7ca1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_SCRATCH_DIR = path(f\"/home/jupyter/scratch/{APP_BASE_NM}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "760f8ece-197a-4c9e-be28-03619594bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca50a5dc-d66c-4ff7-a408-db038568ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88\n",
      "drwxrwxrwx 2 jupyter jupyter  4096 Aug 10 05:18 .\n",
      "drwxr-xr-x 3 jupyter jupyter  4096 Aug 10 05:13 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 40953 Aug 10 05:13 pipeline_3126.json\n",
      "-rw-r--r-- 1 jupyter jupyter 40953 Aug 10 05:18 pipeline_3644.json\n"
     ]
    }
   ],
   "source": [
    "!ls -al $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af5a57b5-4cfc-48cb-9f88-cd437353b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/scratch/customer-churn-model\n"
     ]
    }
   ],
   "source": [
    "!cd $LOCAL_SCRATCH_DIR && pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "593ff06b-8ac6-499d-a988-50b93d0e58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84\n",
      "drwxr-xr-x 12 jupyter jupyter  4096 Aug 10 05:35 .\n",
      "drwxr-xr-x  3 root    root     4096 Aug 10 02:37 ..\n",
      "drwxr-xr-x  5 jupyter jupyter  4096 Aug 10 05:11 .cache\n",
      "drwxr-xr-x  4 jupyter jupyter  4096 Aug 10 02:37 .config\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 Aug 10 02:37 .docker\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 Aug 10 05:12 .ipynb_checkpoints\n",
      "drwxr-xr-x  5 jupyter jupyter  4096 Aug 10 02:47 .ipython\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 Aug 10 02:46 .jupyter\n",
      "drwxr-xr-x  5 jupyter jupyter  4096 Aug 10 05:12 .local\n",
      "-rw-r--r--  1 jupyter jupyter 34653 Aug 10 05:35 customer_churn_training_pipeline.ipynb\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 Aug 10 05:13 scratch\n",
      "drwxr-xr-x  3 jupyter jupyter  4096 Aug 10 02:37 src\n",
      "drwxr-xr-x  4 jupyter jupyter  4096 Aug 10 02:37 tutorials\n"
     ]
    }
   ],
   "source": [
    "!ls -al /home/jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7b7a0-4281-4d6b-9cec-7fed2b300b44",
   "metadata": {},
   "source": [
    "#### d. The pre-created resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "347c9514-0d95-47f7-9671-1c2b5ff73255",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_BUCKET = f\"gs://s8s_code_bucket-{PROJECT_NBR}\"\n",
    "DATA_BUCKET = f\"gs://s8s_data_bucket-{PROJECT_NBR}\"\n",
    "MODEL_BUCKET = f\"gs://s8s_model_bucket-{PROJECT_NBR}\"\n",
    "SCRATCH_BUCKET = f\"s8s-spark-bucket-{PROJECT_NBR}\"\n",
    "BQ_DS_NM = f\"{PROJECT_ID}.customer_churn_ds\"\n",
    "LOCATION = \"us-central1\"\n",
    "VPC_NM = f\"s8s-vpc-{PROJECT_NBR}\"\n",
    "SUBNET_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/subnetworks/spark-snet\"\n",
    "PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/clusters/s8s-sphs-{PROJECT_NBR}\"\n",
    "GCR_REPO_NM = f\"s8s-spark-{PROJECT_NBR}\"\n",
    "DOCKER_IMAGE_TAG = \"1.0.0\"\n",
    "DOCKER_IMAGE_NM = \"customer_churn_image\"\n",
    "DOCKER_IMAGE_FQN = f\"gcr.io/{PROJECT_ID}/{DOCKER_IMAGE_NM}:{DOCKER_IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab3891-5f9d-40f3-b709-80754abe8d1d",
   "metadata": {},
   "source": [
    "#### e. Pipeline entity specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6858b5a-40bd-4705-9a77-2dc3fd97eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ID = 9922\n",
      "PIPELINE_NM = customer-churn-model-pipeline\n",
      "PIPELINE_PACKAGE_SRC_LOCAL_PATH = /home/jupyter/scratch/customer-churn-model/pipeline_9922.json\n",
      "PIPELINE_ROOT_GCS_URI = gs://s8s_model_bucket-505815944775/customer-churn-model/pipelines\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ID = UNIQUE_ID\n",
    "PIPELINE_NM = f\"{APP_BASE_NM}-pipeline\"\n",
    "PIPELINE_PACKAGE_SRC_LOCAL_PATH = f\"{LOCAL_SCRATCH_DIR}/pipeline_{PIPELINE_ID}.json\"\n",
    "PIPELINE_ROOT_GCS_URI = f\"{MODEL_BUCKET}/{APP_BASE_NM}/pipelines\"\n",
    "\n",
    "print('PIPELINE_ID =',PIPELINE_ID)\n",
    "print('PIPELINE_NM =',PIPELINE_NM)\n",
    "print('PIPELINE_PACKAGE_SRC_LOCAL_PATH =',PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "print('PIPELINE_ROOT_GCS_URI =',PIPELINE_ROOT_GCS_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a93fba-9714-46dc-b8f7-929331c159dc",
   "metadata": {},
   "source": [
    "#### d. Pipeline stage agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c437c615-18eb-4aa1-92ef-65a6af02a39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY_SCRIPTS_FQP = gs://s8s_code_bucket-505815944775/pyspark\n",
      "PYSPARK_COMMON_UTILS_SCRIPT_FQP = ['gs://s8s_code_bucket-505815944775/pyspark/common_utils.py']\n"
     ]
    }
   ],
   "source": [
    "PY_SCRIPTS_FQP = f\"{CODE_BUCKET}/pyspark\"\n",
    "PYSPARK_COMMON_UTILS_SCRIPT_FQP = [f\"{PY_SCRIPTS_FQP}/common_utils.py\"]\n",
    "\n",
    "print('PY_SCRIPTS_FQP =',PY_SCRIPTS_FQP)\n",
    "print('PYSPARK_COMMON_UTILS_SCRIPT_FQP =',PYSPARK_COMMON_UTILS_SCRIPT_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf673b2e-89c6-4eaf-bb8b-69b206f84814",
   "metadata": {},
   "source": [
    "#### d. Data preprocessing stage specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d881fbc3-ee62-4969-928d-55a44dd2219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PREPROCESSING_BATCH_INSTANCE_ID = customer-churn-model-preprocessing-9922\n",
      "DATA_PREPROCESSING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-505815944775/pyspark/preprocessing.py\n",
      "DATA_PROCESSING_SINK = spark-s8s-mlops.customer_churn_ds.training_data\n",
      "DATA_PROCESSING_BQ_SINK_URI = bq://spark-s8s-mlops.customer_churn_ds.training_data\n",
      "DATA_PREPROCESSING_ARGS = ['--pipelineID=9922', '--projectID=spark-s8s-mlops', '--projectNbr=505815944775', '--displayPrintStatements=True']\n"
     ]
    }
   ],
   "source": [
    "DATA_PREPROCESSING_BATCH_PREFIX = \"preprocessing\"\n",
    "DATA_PREPROCESSING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{DATA_PREPROCESSING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "DATA_PREPROCESSING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/preprocessing.py\"\n",
    "\n",
    "DATA_PROCESSING_SINK = f\"{BQ_DS_NM}.training_data\"\n",
    "DATA_PROCESSING_BQ_SINK_URI = f\"bq://{DATA_PROCESSING_SINK}\"\n",
    "\n",
    "DATA_PREPROCESSING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "print('DATA_PREPROCESSING_BATCH_INSTANCE_ID =',DATA_PREPROCESSING_BATCH_INSTANCE_ID)\n",
    "print('DATA_PREPROCESSING_MAIN_PY_SCRIPT =',DATA_PREPROCESSING_MAIN_PY_SCRIPT)\n",
    "print('DATA_PROCESSING_SINK =',DATA_PROCESSING_SINK)\n",
    "print('DATA_PROCESSING_BQ_SINK_URI =',DATA_PROCESSING_BQ_SINK_URI)\n",
    "print('DATA_PREPROCESSING_ARGS =',DATA_PREPROCESSING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca656c-5ae8-45ab-95a3-873fede76c01",
   "metadata": {},
   "source": [
    "#### e. Dataset registration specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50eae873-c8e8-4f0b-9bac-e83c58f64ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANAGED_DATASET_NM = f\"{APP_BASE_NM}-{UNIQUE_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d69e4b-d088-4293-a88e-5b4d943615c8",
   "metadata": {},
   "source": [
    "#### f. Model specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecfd7e0e-3b57-4c4d-a148-4357bf816b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_TRAINING_BATCH_INSTANCE_ID = customer-churn-model-training-9922\n",
      "MODEL_TRAINING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-505815944775/pyspark/model_training.py\n",
      "MODEL_TRAINING_ARGS = ['--pipelineID=9922', '--projectID=spark-s8s-mlops', '--projectNbr=505815944775', '--displayPrintStatements=True']\n",
      "MODEL_METRICS_BUCKET_FQP = gs://s8s_metrics_bucket-505815944775/customer-churn-model/training/9922/full/metrics.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINING_BATCH_PREFIX = \"training\"\n",
    "MODEL_TRAINING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{MODEL_TRAINING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "MODEL_TRAINING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/model_training.py\"\n",
    "MODEL_TRAINING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "MODEL_METRICS_BUCKET_FQP = f\"gs://s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{MODEL_TRAINING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "\n",
    "print('MODEL_TRAINING_BATCH_INSTANCE_ID =',MODEL_TRAINING_BATCH_INSTANCE_ID)\n",
    "print('MODEL_TRAINING_MAIN_PY_SCRIPT =',MODEL_TRAINING_MAIN_PY_SCRIPT)\n",
    "print('MODEL_TRAINING_ARGS =',MODEL_TRAINING_ARGS)\n",
    "print('MODEL_METRICS_BUCKET_FQP =',MODEL_METRICS_BUCKET_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8f31c-44e3-44cc-8eb1-9eadf50c0137",
   "metadata": {},
   "source": [
    "#### g. Hyperparameter tuning specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bfbc78c-8d1d-4590-a86d-925b35c3fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID = customer-churn-model-hyperparameter-tuning-9922\n",
      "HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT = gs://s8s_code_bucket-505815944775/pyspark/hyperparameter_tuning.py\n",
      "HYPERPARAMETER_TUNING_ARGS = ['--pipelineID=9922', '--projectID=spark-s8s-mlops', '--projectNbr=505815944775', '--displayPrintStatements=True']\n"
     ]
    }
   ],
   "source": [
    "# Condition\n",
    "AUPR_THRESHOLD = 0.5\n",
    "AUPR_HYPERTUNE_CONDITION = \"[AUPR_HYPERTUNE]\"\n",
    "\n",
    "HYPERPARAMETER_TUNING_BATCH_PREFIX = \"hyperparameter-tuning\"\n",
    "HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID = f\"{APP_BASE_NM}-{HYPERPARAMETER_TUNING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "HYPERPARAMETER_TUNING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/hyperparameter_tuning.py\"\n",
    "HYPERPARAMETER_TUNING_BUCKET_FQP = f\"gs://s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{HYPERPARAMETER_TUNING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "\n",
    "\n",
    "print('HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID =',HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID)\n",
    "print('HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT =',HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT)\n",
    "print('HYPERPARAMETER_TUNING_ARGS =',HYPERPARAMETER_TUNING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e5090-1f28-4022-b3b1-82099710238b",
   "metadata": {},
   "source": [
    "### 2. Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9c54e20-c08a-40cf-8716-09de594df623",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=SCRATCH_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4fd3-4b64-4946-a4ac-5937ad96a671",
   "metadata": {},
   "source": [
    "### 3. Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af80172f-b441-4104-98a0-3bf42dff4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"numpy==1.21.2\", \"pandas==1.3.3\", \"scikit-learn==0.24.2\"],\n",
    ")\n",
    "def fnEvaluateModel(\n",
    "    metricsUri: str,\n",
    "    metrics: Output[Metrics],\n",
    "    plots: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"threshold_metric\", float)]):\n",
    "\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "    # Variables\n",
    "    metricsGCSMountPath = metricsUri.replace(\"gs://\", \"/gcs/\")\n",
    "    labels = [\"yes\", \"no\"]\n",
    "\n",
    "    # Helpers\n",
    "    def fnCalculateROC(metrics, true, score):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_score_np = np.array(metrics[score])\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true=y_true_np, y_score=y_score_np, pos_label=True\n",
    "        )\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def fnCalculateConfusionMatrix(metrics, true, prediction):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_pred_np = np.array(metrics[prediction])\n",
    "        c_matrix = confusion_matrix(y_true_np, y_pred_np)\n",
    "        return c_matrix\n",
    "\n",
    "    # Main\n",
    "    with open(metricsGCSMountPath, mode=\"r\") as json_file:\n",
    "        metricsDictionary = json.load(json_file)\n",
    "\n",
    "    area_roc = metricsDictionary[\"test_area_roc\"]\n",
    "    area_prc = metricsDictionary[\"test_area_prc\"]\n",
    "    acc = metricsDictionary[\"test_accuracy\"]\n",
    "    f1 = metricsDictionary[\"test_f1\"]\n",
    "    prec = metricsDictionary[\"test_precision\"]\n",
    "    rec = metricsDictionary[\"test_recall\"]\n",
    "\n",
    "    metrics.log_metric(\"Test_areaUnderROC\", area_roc)\n",
    "    metrics.log_metric(\"Test_areaUnderPRC\", area_prc)\n",
    "    metrics.log_metric(\"Test_Accuracy\", acc)\n",
    "    metrics.log_metric(\"Test_f1-score\", f1)\n",
    "    metrics.log_metric(\"Test_Precision\", prec)\n",
    "    metrics.log_metric(\"Test_Recall\", rec)\n",
    "\n",
    "    fpr, tpr, thresholds = fnCalculateROC(metricsDictionary, \"true\", \"score\")\n",
    "    c_matrix = fnCalculateConfusionMatrix(metricsDictionary, \"true\", \"prediction\")\n",
    "    plots.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    plots.log_confusion_matrix(labels, c_matrix.tolist())\n",
    "\n",
    "    componentOutputsTuple = NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"threshold_metric\", float),\n",
    "        ],\n",
    "    )\n",
    "    return componentOutputsTuple(area_prc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3c111-17dc-4248-9df8-7c65f73c696c",
   "metadata": {},
   "source": [
    "### 4. Define Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "346cdd5b-53a4-4760-b4f7-e595f7c20d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A SparkMLlib MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args\n",
    "    ).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    \n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    createManagedDatasetStep = vertex_ai_components.TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args\n",
    "    ).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    # Step 4. EVALUATE MODEL\n",
    "    # .................................................................... \n",
    "    evaluateModelStep = fnEvaluateModel(model_training_metrics_fqp).after(trainSparkMLModelStep).set_display_name(\"Evaluate model\")\n",
    "    \n",
    "    # Step 5. CONDITIONAL HYPERPARAMETER TUNING\n",
    "    # .................................................................... \n",
    "    with Condition(\n",
    "        evaluateModelStep.outputs[\"threshold_metric\"] >= threshold,\n",
    "        name=\"AUPR Threshold Exceeded\",\n",
    "    ):\n",
    "        # HYPERPARAMETER TUNING\n",
    "        hyperparameterTuningStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = hyperparameter_tuning_pyspark_batch_id,\n",
    "        main_python_file_uri = hyperparameter_tuning_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = hyperparameter_tuning_args\n",
    "        ).after(evaluateModelStep).set_display_name(\"Hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb932bdd-8e1d-4a04-84bd-10bb6d7bd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A SparkMLlib MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipelineWithoutCaching(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args\n",
    "    ).set_caching_options(False).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    \n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    createManagedDatasetStep = vertex_ai_components.TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_caching_options(False).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args\n",
    "    ).set_caching_options(False).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    # Step 4. EVALUATE MODEL\n",
    "    # .................................................................... \n",
    "    evaluateModelStep = fnEvaluateModel(model_training_metrics_fqp).after(trainSparkMLModelStep).set_caching_options(False).set_display_name(\"Evaluate model\")\n",
    "    \n",
    "    # Step 5. CONDITIONAL HYPERPARAMETER TUNING\n",
    "    # .................................................................... \n",
    "    with Condition(\n",
    "        evaluateModelStep.outputs[\"threshold_metric\"] >= threshold,\n",
    "        name=\"AUPR Threshold Exceeded\",\n",
    "    ):\n",
    "        # HYPERPARAMETER TUNING\n",
    "        hyperparameterTuningStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = hyperparameter_tuning_pyspark_batch_id,\n",
    "        main_python_file_uri = hyperparameter_tuning_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = hyperparameter_tuning_args\n",
    "        ).after(evaluateModelStep).set_caching_options(False).set_display_name(\"Hyperparameter tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126d252-8cad-4615-bb65-1126792ef9e7",
   "metadata": {},
   "source": [
    "### 4. Compile the Vertex AI Pipeline into a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb6ecfbc-dcad-4848-9acf-de6687c91701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing fnSparkMlopsPipelineWithoutCaching\n"
     ]
    }
   ],
   "source": [
    "if WITHOUT_TASK_CACHING:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipelineWithoutCaching, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipelineWithoutCaching\")\n",
    "else:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipeline, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f665f0d-7a95-4dfb-b210-a959a3a3f914",
   "metadata": {},
   "source": [
    "### 5. Submit the Pipeline for execution via Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "156f8be4-5369-4078-8191-96ca42eeb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NM,\n",
    "    template_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT_GCS_URI,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93c96b7f-ab01-4546-9eec-d53aa1800c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/505815944775/locations/us-central1/pipelineJobs/customer-churn-model-pipeline-20220810053536\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/505815944775/locations/us-central1/pipelineJobs/customer-churn-model-pipeline-20220810053536')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/customer-churn-model-pipeline-20220810053536?project=505815944775\n"
     ]
    }
   ],
   "source": [
    "if BYO_NETWORK:\n",
    "    pipeline.submit(service_account=UMSA_FQN, network=f\"projects/{PROJECT_NBR}/global/networks/{VPC_NM}\")\n",
    "else:\n",
    "    pipeline.submit(service_account=UMSA_FQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f572b16-1221-4ed7-8e4c-ed52232a48f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
