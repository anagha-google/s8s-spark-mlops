{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamater tuning\n",
    "This script does hyperparameter tuning of a Random Forest Classification model for the customer churn prediction experiment-</br>\n",
    "1. Reads source data from BigQuery as a source, \n",
    "2. Writes model to GCS\n",
    "3. Captures and persists model metrics to GCS and BigQuery\n",
    "4. Writes model test results to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path as path\n",
    "from google.cloud import storage\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Arguments\n",
    "pipelineID = \"20220813\"\n",
    "projectNbr = \"974925525028\"\n",
    "projectID = \"s8s-spark-ml-mlops\"\n",
    "displayPrintStatements = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Variables \n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"hyperparameter-tuning\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "modelVersion = pipelineID\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data\"\n",
    "bigQueryModelTestResultsTableFQN = f\"{bqDatasetNm}.test_predictions\"\n",
    "bigQueryModelMetricsTableFQN = f\"{bqDatasetNm}.model_metrics\"\n",
    "modelBucketUri = f\"gs://s8s_model_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "metricsBucketUri = f\"gs://s8s_metrics_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/pipelineId-{pipelineID}/{appNameSuffix}/\"\n",
    "pipelineExecutionDt = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other variables, constants\n",
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]\n",
    "MAX_DEPTH = [5, 10, 15]\n",
    "MAX_BINS = [24, 32, 40]\n",
    "N_TREES = [25, 30, 35]\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Display input and output\n",
    "if displayPrintStatements:\n",
    "    print(\"Starting hyperparameter tuning for *Customer Churn* experiment\")\n",
    "    print(\".....................................................\")\n",
    "    print(f\"The datetime now is - {pipelineExecutionDt}\")\n",
    "    print(\" \")\n",
    "    print(\"INPUT PARAMETERS\")\n",
    "    print(f\"....pipelineID={pipelineID}\")\n",
    "    print(f\"....projectID={projectID}\")\n",
    "    print(f\"....projectNbr={projectNbr}\")\n",
    "    print(f\"....displayPrintStatements={displayPrintStatements}\")\n",
    "    print(\" \")\n",
    "    print(\"EXPECTED SETUP\")  \n",
    "    print(f\"....BQ Dataset={bqDatasetNm}\")\n",
    "    print(f\"....Model Training Source Data in BigQuery={bigQuerySourceTableFQN}\")\n",
    "    print(f\"....Scratch Bucket for BQ connector=gs://s8s-spark-bucket-{projectNbr}\") \n",
    "    print(f\"....Model Bucket=gs://s8s-model-bucket-{projectNbr}\")  \n",
    "    print(f\"....Metrics Bucket=gs://s8s-metrics-bucket-{projectNbr}\") \n",
    "    print(\" \")\n",
    "    print(\"OUTPUT\")\n",
    "    print(f\"....Model in GCS={modelBucketUri}\")\n",
    "    print(f\"....Model metrics in GCS={metricsBucketUri}\")  \n",
    "    print(f\"....Model metrics in BigQuery={bigQueryModelMetricsTableFQN}\")      \n",
    "    print(f\"....Model test results in BigQuery={bigQueryModelTestResultsTableFQN}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark config\n",
    "print('....Setting Spark config')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "# Spark configuration setting for writes to BigQuery\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Add Python modules\n",
    "sc.addPyFile(f\"gs://s8s_code_bucket-{projectNbr}/pyspark/common_utils.py\")\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pre-process training data\n",
    "print('....Data pre-procesing')\n",
    "dataPreprocessingStagesList = []\n",
    "# 3a. Create and append to pipeline stages - string indexing and one hot encoding\n",
    "for eachCategoricalColumn in common_utils.CATEGORICAL_COLUMN_LIST:\n",
    "    # Category indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=eachCategoricalColumn, outputCol=eachCategoricalColumn + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    # Add stages.  This is a lazy operation\n",
    "    dataPreprocessingStagesList += [stringIndexer, encoder]\n",
    "\n",
    "# 3b. Convert label into label indices using the StringIndexer and append to pipeline stages\n",
    "labelStringIndexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "dataPreprocessingStagesList += [labelStringIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature engineering\n",
    "print('....Feature engineering')\n",
    "featureEngineeringStageList = []\n",
    "assemblerInputs = common_utils.NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in common_utils.CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "featureEngineeringStageList += [featuresVectorAssembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model training\n",
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "modelTrainingStageList += [rfClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a model training pipeline for stages defined\n",
    "print('....Instantiating pipeline model')\n",
    "pipeline = Pipeline(stages=dataPreprocessingStagesList + featureEngineeringStageList + modelTrainingStageList)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Hyperparameter tuning & cross validation\n",
    "print('....Hyperparameter tuning & cross validation')\n",
    "parameterGrid = (ParamGridBuilder()\n",
    "               .addGrid(modelTrainingStageList[0].maxDepth, MAX_DEPTH)\n",
    "               .addGrid(modelTrainingStageList[0].maxBins, MAX_BINS)\n",
    "               .addGrid(modelTrainingStageList[0].numTrees, N_TREES)\n",
    "               .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "crossValidatorPipeline = CrossValidator(estimator=pipeline,\n",
    "                                 estimatorParamMaps=parameterGrid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Read training data\n",
    "print('....Reading training dataset')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "\n",
    "\n",
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"partner\", inputDF.partner.cast('string')) \\\n",
    "    .withColumn(\"dependents\", inputDF.dependents.cast('string')) \\\n",
    "    .withColumn(\"phone_service\", inputDF.phone_service.cast('string')) \\\n",
    "    .withColumn(\"paperless_billing\", inputDF.paperless_billing.cast('string')) \\\n",
    "    .withColumn(\"churn\", inputDF.churn.cast('string')) \\\n",
    "    .withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Split to training and test datasets\n",
    "print('....Splitting the dataset')\n",
    "trainDF, testDF = inputDF.randomSplit(SPLIT_SPECS, seed=SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the model; Takes tens of minutes, repartition as needed\n",
    "trainDF.repartition(300)\n",
    "pipelineModel = crossValidatorPipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Persist model to GCS\n",
    "pipelineModel.write().overwrite().save(modelBucketUri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Test the model with the test dataset\n",
    "print('....Testing the model')\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "predictionsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Persist model testing results to BigQuery\n",
    "predictionsWithPipelineIdDF = predictionsDF.withColumn(\"pipeline_id\", lit(pipelineID)) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID)) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "predictionsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', bigQueryModelTestResultsTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnCaptureModelMetrics(predictionsDF, labelColumn, operation):\n",
    "    \"\"\"\n",
    "    Get model metrics\n",
    "    Args:\n",
    "        predictions: predictions\n",
    "        labelColumn: target column\n",
    "        operation: train or test\n",
    "    Returns:\n",
    "        metrics: metrics\n",
    "        \n",
    "    Anagha TODO: This function if called from common_utils fails; Need to researchy why\n",
    "    \"\"\"\n",
    "    \n",
    "    metricLabels = ['area_roc', 'area_prc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    metricColumns = ['true', 'score', 'prediction']\n",
    "    metricKeys = [f'{operation}_{ml}' for ml in metricLabels] + metricColumns\n",
    "\n",
    "    # Instantiate evaluators\n",
    "    bcEvaluator = BinaryClassificationEvaluator(labelCol=labelColumn)\n",
    "    mcEvaluator = MulticlassClassificationEvaluator(labelCol=labelColumn)\n",
    "\n",
    "    # Capture metrics -> areas, acc, f1, prec, rec\n",
    "    area_roc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderROC'}), 5)\n",
    "    area_prc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderPR'}), 5)\n",
    "    acc = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"accuracy\"}), 5)\n",
    "    f1 = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"f1\"}), 5)\n",
    "    prec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedPrecision\"}), 5)\n",
    "    rec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedRecall\"}), 5)\n",
    "\n",
    "    # Get the true, score, prediction off of the test results dataframe\n",
    "    rocDictionary = common_utils.fnGetTrueScoreAndPrediction(predictionsDF, labelColumn)\n",
    "    true = rocDictionary['true']\n",
    "    score = rocDictionary['score']\n",
    "    prediction = rocDictionary['prediction']\n",
    "\n",
    "    # Create a metric values array\n",
    "    metricValuesArray = [] \n",
    "    metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec))\n",
    "    #metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec, true, score, prediction))\n",
    "    \n",
    "    # Zip the keys and values into a dictionary  \n",
    "    metricsDictionary = dict(zip(metricKeys, metricValuesArray))\n",
    "\n",
    "    return metricsDictionary\n",
    "# }} End fnCaptureModelmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Capture & display metrics\n",
    "hyperParameterTunedModelMetrics = fnCaptureModelMetrics(predictionsDF, \"label\", \"test\")\n",
    "for m, v in hyperParameterTunedModelMetrics.items():\n",
    "    print(f'{m}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Persist metrics to BigQuery\n",
    "metricsDF = spark.createDataFrame(hyperParameterTunedModelMetrics.items(), [\"metric_nm\", \"metric_value\"]) \n",
    "metricsWithPipelineIdDF = metricsDF.withColumn(\"pipeline_id\", lit(pipelineID)) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID)) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "metricsWithPipelineIdDF.show()\n",
    "metricsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', bigQueryModelMetricsTableFQN) \\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Persist metrics to GCS\n",
    "blobName = f\"{modelBaseNm}/{operation}/{modelVersion}/metrics.json\"\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, hyperParameterTunedModelMetrics, blobName)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/avani-2\",\"uuid\":\"7e7683cb-2249-412f-ba67-cc6705d530d8\",\"createTime\":\"2022-08-02T04:25:00.827599Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-02T04:25:44.676645Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/7e7683cb-2249-412f-ba67-cc6705d530d8/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-02T04:25:00.827599Z\"}]}",
  "serverless_spark_kernel_name": "remote-d5cec8953170e40cda62aec7-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
